{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2026.01"
      },
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaFZ/-/blob/main/owon_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7Dknl4nBcp0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y unsloth unsloth_zoo\n",
        "!pip install --no-cache-dir -U \\\n",
        "  git+https://github.com/unslothai/unsloth.git \\\n",
        "  git+https://github.com/unslothai/unsloth-zoo.git \\\n",
        "  trl peft accelerate bitsandbytes datasets\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tDNuXNJIm5Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# 6. é…ç½®æ¨¡å‹å‚æ•°\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# 7. åŠ è½½æ¨¡å‹\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 8. è½¬æ¢æ¨¡å‹ä¸º LoRA æ¨¡å¼\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")"
      ],
      "metadata": {
        "id": "ItyQ-kclEM7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æœ‰é—®é¢˜ä»£ç ï¼Œå…ˆç•™ç€\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 5. å®šä¹‰æ•°æ®å¤„ç†æ ¼å¼\n",
        "# å°†JSONL æ ¼å¼è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¬æ‡‚çš„ Prompt æ ¼å¼\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a Response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # å¥å­ç»“æŸç¬¦\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # å¡«å……æ¨¡æ¿\n",
        "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# 6. åŠ è½½å¹¶å¤„ç†æ•°æ®é›†\n",
        "# ç¡®ä¿ä½ çš„æ–‡ä»¶åæ˜¯ 'train.jsonl'\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/train.jsonl\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)"
      ],
      "metadata": {
        "id": "6FLxyziQEzz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ç°åœ¨è°ƒè¯•ç”¨ä»£ç \n",
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. æ”¹å›æ ‡å‡† Alpaca æ¨¡æ¿ (ä½¿ç”¨ ### Response:)\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘æ¸…æ´—æ•°æ®ï¼šå¦‚æœ output æ˜¯å­—å…¸ï¼Œå»æ‰æ‰€æœ‰å€¼ä¸º None çš„é”®\n",
        "        if isinstance(output, dict):\n",
        "            # è¿™ä¸€è¡Œæ˜¯å…³é”®ï¼šåªä¿ç•™å€¼ä¸ä¸º None çš„å­—æ®µ\n",
        "            clean_output = {k: v for k, v in output.items() if v is not None}\n",
        "            output_str = json.dumps(clean_output, ensure_ascii=False)\n",
        "        else:\n",
        "            output_str = str(output)\n",
        "\n",
        "        text = alpaca_prompt.format(instruction, input, output_str) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# åŠ è½½æ•°æ®\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/train.jsonl\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "# 3. ã€æ–°å¢ã€‘æ‰“å°ä¸€æ¡å¤„ç†åçš„æ•°æ®ï¼Œæ£€æŸ¥æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼\n",
        "print(\"æ£€æŸ¥ç¬¬ä¸€æ¡è®­ç»ƒæ•°æ®æ ¼å¼ï¼š\")\n",
        "print(dataset[\"text\"][0])\n",
        "# åŠ¡å¿…ç¡®è®¤è¿™é‡Œçœ‹åˆ°çš„æ˜¯ {\"intent\": ...} (åŒå¼•å·)ï¼Œè€Œä¸æ˜¯ {'intent': ...} (å•å¼•å·)"
      ],
      "metadata": {
        "id": "Q1nZ7b1xarMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4, # æ˜¾å­˜å…è®¸çš„è¯ï¼Œè¶Šå°æ›´æ–°è¶Šé¢‘ç¹\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # max_steps = 60,  <-- åˆ æ‰è¿™ä¸ª\n",
        "        num_train_epochs = 15, # ã€ä¿®æ”¹ã€‘ç›´æ¥æŒ‡å®šè·‘ 15 è½®ï¼Œç¡®ä¿å­¦ä¼šæ ¼å¼\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fAai4pEYbNWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# å¿…é¡»ä½¿ç”¨å®Œå…¨ä¸€è‡´çš„ Prompt æ¨¡æ¿\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"ä½ æ˜¯ä¸€ä¸ªå·¥ä¸šçº§ç¤ºæ³¢å™¨æŒ‡ä»¤è§£æå¼•æ“ã€‚ä½ å¿…é¡»åªè¾“å‡º JSONï¼Œä¸å¾—åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡æœ¬ã€‚\",\n",
        "        \"æŠŠé€šé“2çš„æ³¢å½¢å­˜ä¸‹æ¥\",\n",
        "        \"\", # è¿™é‡Œç•™ç©º\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "# è§£ç æ—¶è·³è¿‡ prompt éƒ¨åˆ†\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "kZBj1TsNbULe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# å»ºè®®åœ¨ Drive é‡Œå»ºä¸€ä¸ªä¸“é—¨çš„æ–‡ä»¶å¤¹ï¼Œæ¯”å¦‚ 'oscilloscope_project'\n",
        "import os\n",
        "save_path = \"/content/drive/MyDrive/oscilloscope_project/lora_model\"\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"âœ… æ¨¡å‹å·²å®‰å…¨ä¿å­˜åˆ° Google Drive: {save_path}\")"
      ],
      "metadata": {
        "id": "-eu4XZZ7o0Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ç¡®ä¿å·²æŒ‚è½½ Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. å®šä¹‰ Google Drive ä¸­çš„ä¿å­˜è·¯å¾„\n",
        "# å»ºè®®ä¿å­˜åœ¨ä¸“å±æ–‡ä»¶å¤¹ä¸­ï¼Œæ–¹ä¾¿ä¸‹è½½åˆ° Ubuntu\n",
        "save_directory = \"/content/drive/MyDrive/oscilloscope_project/qwen_merged_hf\"\n",
        "\n",
        "print(f\"æ­£åœ¨åˆå¹¶æ¨¡å‹å¹¶ä¿å­˜è‡³: {save_directory} ...\")\n",
        "\n",
        "# 3. åˆå¹¶å¹¶å¯¼å‡ºä¸º 16bit æ ¼å¼ (RKLLM è½¬æ¢çš„æœ€ä½³å…¼å®¹æ ¼å¼)\n",
        "model.save_pretrained_merged(\n",
        "    save_directory,\n",
        "    tokenizer,\n",
        "    save_method = \"merged_16bit\",\n",
        ")\n",
        "\n",
        "print(\"âœ… æ¨¡å‹åˆå¹¶å®Œæˆï¼ä½ ç°åœ¨å¯ä»¥åœ¨ Google Drive ç½‘é¡µç«¯çœ‹åˆ° qwen_merged_hf æ–‡ä»¶å¤¹ã€‚\")\n",
        "!zip -r merged_model.zip {save_directory}\n",
        "#"
      ],
      "metadata": {
        "id": "tLCYPhM3pnSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0b81104"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('merged_model.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a4a8684"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. æŒ‚è½½ Google Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# 2. å®šä¹‰ä¿å­˜ç¯å¢ƒçš„ Google Drive è·¯å¾„\n",
        "lib_path = \"/content/drive/MyDrive/colab_env_libs\"\n",
        "if not os.path.exists(lib_path):\n",
        "    os.makedirs(lib_path)\n",
        "\n",
        "# 3. å°†è¯¥è·¯å¾„åŠ å…¥åˆ° Python çš„ç³»ç»Ÿè·¯å¾„ä¸­ (ä¼˜å…ˆäºç³»ç»Ÿåº“)\n",
        "if lib_path not in sys.path:\n",
        "    sys.path.insert(0, lib_path)\n",
        "\n",
        "print(f\"âœ… å·²åŠ è½½å¤–éƒ¨åº“è·¯å¾„: {lib_path}\")\n",
        "\n",
        "# 4. å®‰è£…/æ›´æ–°ç¯å¢ƒåˆ° Drive\n",
        "# ã€å…³é”®ä¿®æ”¹ã€‘æ”¹ä¸º Falseï¼Œä¸‹æ¬¡æ£€æµ‹åˆ°æ–‡ä»¶å­˜åœ¨å°±ä¼šè‡ªåŠ¨è·³è¿‡å®‰è£…ï¼ŒèŠ‚çœæ—¶é—´\n",
        "FORCE_UPDATE = False\n",
        "\n",
        "target_exists = os.path.exists(os.path.join(lib_path, \"unsloth\"))\n",
        "\n",
        "if target_exists and not FORCE_UPDATE:\n",
        "    print(\"âœ… æ£€æµ‹åˆ° Google Drive ä¸­å·²å­˜åœ¨ Unsloth ç¯å¢ƒï¼Œè·³è¿‡å®‰è£…æ­¥éª¤ï¼\")\n",
        "    print(\"   (å¦‚éœ€å¼ºåˆ¶æ›´æ–°ï¼Œè¯·å°†ä»£ç ä¸­çš„ FORCE_UPDATE æ”¹ä¸º True)\")\n",
        "else:\n",
        "    if FORCE_UPDATE:\n",
        "        print(\"ğŸ”„ æ­£åœ¨å¼ºåˆ¶æ›´æ–° Google Drive ä¸­çš„ç¯å¢ƒ (å‚è€ƒæœ€æ–°ä¿®å¤é…ç½®)...\")\n",
        "    else:\n",
        "        print(\"âš¡ï¸ Drive ä¸­æœªæ£€æµ‹åˆ°ç¯å¢ƒï¼Œæ­£åœ¨å®‰è£…...\")\n",
        "\n",
        "    # æ·»åŠ  --no-input å‚æ•°ä»¥ç¦æ­¢å¼¹å‡ºå¯¹è¯æ¡†/ç¡®è®¤æç¤º\n",
        "\n",
        "    # 1. å®‰è£… unsloth_zoo\n",
        "    !pip install --no-input --upgrade --no-cache-dir --no-deps --target={lib_path} git+https://github.com/unslothai/unsloth-zoo.git\n",
        "\n",
        "    # 2. å®‰è£… unsloth ä¸»ç¨‹åº\n",
        "    !pip install --no-input --upgrade --no-cache-dir --target={lib_path} \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "\n",
        "    # 3. å®‰è£…å…¶ä»–ä¾èµ–\n",
        "    !pip install --no-input --no-deps --target={lib_path} xformers \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "\n",
        "    print(\"ğŸ‰ ç¯å¢ƒå·²æˆåŠŸå®‰è£…/æ›´æ–°åˆ° Google Driveï¼ä¸‹æ¬¡å¯åŠ¨å¯ç›´æ¥åŠ è½½ã€‚\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcb298b0"
      },
      "source": [
        "print(f\"æ­£åœ¨ä» {save_directory} åŠ è½½åˆå¹¶åçš„æ¨¡å‹...\")\n",
        "\n",
        "# ç”±äºæ˜¯åˆå¹¶åçš„æ¨¡å‹ï¼Œå¯ä»¥ç›´æ¥åŠ è½½ä¸ºæ ‡å‡†çš„ Hugging Face æ¨¡å‹\n",
        "# æ³¨æ„ï¼šæ­¤å¤„ä½¿ç”¨çš„ FastLanguageModel.from_pretrained æ˜¯é’ˆå¯¹ Unsloth ä¼˜åŒ–è¿‡çš„æ¨¡å‹åŠ è½½æ–¹å¼\n",
        "# å¯¹äºå®Œå…¨åˆå¹¶çš„æ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ AutoModelForCausalLM.from_pretrained\n",
        "# ä½†ä¸ºäº†ä¿æŒä¸€è‡´æ€§ï¼Œæˆ‘ä»¬ç»§ç»­ä½¿ç”¨ FastLanguageModel\n",
        "merged_model, merged_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = save_directory, # æŒ‡å‘ä¿å­˜åˆå¹¶æ¨¡å‹çš„ç›®å½•\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = None, # è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹\n",
        "    load_in_4bit = False, # åˆå¹¶åçš„æ¨¡å‹é€šå¸¸ä¸éœ€è¦å†è¿›è¡Œ 4bit é‡åŒ–åŠ è½½\n",
        ")\n",
        "\n",
        "print(\"âœ… åˆå¹¶æ¨¡å‹åŠ è½½å®Œæˆï¼\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æœ‰é—®é¢˜ï¼Œå…ˆä¿ç•™\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# 7. è®¾ç½®è®­ç»ƒå‚æ•°\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # å¦‚æœæ•°æ®é‡å¾ˆå¤§å¯ä»¥è®¾ä¸º True åŠ é€Ÿ\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4, # é™ä½ Batch Size\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 10,\n",
        "        # max_steps = 60,  <-- åˆ é™¤è¿™ä¸€è¡Œ\n",
        "        num_train_epochs = 10, # <-- æ˜¾å¼æŒ‡å®šè·‘ 10 åˆ° 15 è½®\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"cosine\", # å»ºè®®ç”± linear æ”¹ä¸º cosine\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "# 8. å¼€å§‹è®­ç»ƒ\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "xmg0ehZcGi_s",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æœ‰é—®é¢˜ï¼Œå…ˆä¿ç•™\n",
        "FastLanguageModel.for_inference(model) # å¼€å¯æ¨ç†æ¨¡å¼\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"ä½ æ˜¯ä¸€ä¸ªå·¥ä¸šçº§ç¤ºæ³¢å™¨æŒ‡ä»¤è§£æå¼•æ“ã€‚ä½ å¿…é¡»åªè¾“å‡º JSONï¼Œä¸å¾—åŒ…å«ä»»ä½•è§£é‡Šæ€§æ–‡æœ¬ã€‚\",\n",
        "        \"å¼€å¯ZOOM\",\n",
        "        \"{\", # <--- å…³é”®ä¿®æ”¹ï¼šé¢„å¡«å……ä¸€ä¸ªå·¦å¤§æ‹¬å·\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "print(tokenizer.batch_decode(outputs))"
      ],
      "metadata": {
        "id": "HQB4ydX-G86l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}