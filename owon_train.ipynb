{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2026.01"
      },
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshuaFZ/-/blob/main/owon_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7Dknl4nBcp0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y unsloth unsloth_zoo\n",
        "!pip install --no-cache-dir -U \\\n",
        "  git+https://github.com/unslothai/unsloth.git \\\n",
        "  git+https://github.com/unslothai/unsloth-zoo.git \\\n",
        "  trl peft accelerate bitsandbytes datasets\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "tDNuXNJIm5Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# 6. 配置模型参数\n",
        "max_seq_length = 2048\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "# 7. 加载模型\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# 8. 转换模型为 LoRA 模式\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        ")"
      ],
      "metadata": {
        "id": "ItyQ-kclEM7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 现在调试用代码\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "\n",
        "# 1. 改回标准 Alpaca 模板 (使用 ### Response:)\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs       = examples[\"input\"]\n",
        "    outputs      = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "        # 【核心修改】清洗数据：如果 output 是字典，去掉所有值为 None 的键\n",
        "        if isinstance(output, dict):\n",
        "            # 这一行是关键：只保留值不为 None 的字段\n",
        "            clean_output = {k: v for k, v in output.items() if v is not None}\n",
        "            output_str = json.dumps(clean_output, ensure_ascii=False)\n",
        "        else:\n",
        "            output_str = str(output)\n",
        "\n",
        "        text = alpaca_prompt.format(instruction, input, output_str) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# 加载数据\n",
        "dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/train.jsonl\", split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)\n",
        "\n",
        "# 3. 【新增】打印一条处理后的数据，检查格式是否正确！\n",
        "print(\"检查第一条训练数据格式：\")\n",
        "print(dataset[\"text\"][0])\n",
        "# 务必确认这里看到的是 {\"intent\": ...} (双引号)，而不是 {'intent': ...} (单引号)"
      ],
      "metadata": {
        "id": "Q1nZ7b1xarMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 4, # 显存允许的话，越小更新越频繁\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        # max_steps = 60,  <-- 删掉这个\n",
        "        num_train_epochs = 15, # 【修改】直接指定跑 15 轮，确保学会格式\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fAai4pEYbNWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "# 必须使用完全一致的 Prompt 模板\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"你是一个工业级示波器指令解析引擎。你必须只输出 JSON，不得包含任何解释性文本。\",\n",
        "        \"将触发源改为外部\",\n",
        "        \"\", # 这里留空\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "# 解码时跳过 prompt 部分\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "kZBj1TsNbULe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 建议在 Drive 里建一个专门的文件夹，比如 'oscilloscope_project'\n",
        "import os\n",
        "save_path = \"/content/drive/MyDrive/oscilloscope_project/lora_model\"\n",
        "\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "print(f\"✅ 模型已安全保存到 Google Drive: {save_path}\")"
      ],
      "metadata": {
        "id": "-eu4XZZ7o0Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 确保已挂载 Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. 定义 Google Drive 中的保存路径\n",
        "# 建议保存在专属文件夹中，方便下载到 Ubuntu\n",
        "save_directory = \"/content/drive/MyDrive/oscilloscope_project/qwen_merged_hf\"\n",
        "\n",
        "print(f\"正在合并模型并保存至: {save_directory} ...\")\n",
        "\n",
        "# 3. 合并并导出为 16bit 格式 (RKLLM 转换的最佳兼容格式)\n",
        "model.save_pretrained_merged(\n",
        "    save_directory,\n",
        "    tokenizer,\n",
        "    save_method = \"merged_16bit\",\n",
        ")\n",
        "\n",
        "print(\"✅ 模型合并完成！你现在可以在 Google Drive 网页端看到 qwen_merged_hf 文件夹。\")"
      ],
      "metadata": {
        "id": "tLCYPhM3pnSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0b81104"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "save_directory = \"/content/drive/MyDrive/oscilloscope_project/qwen_merged_hf\"\n",
        "!zip -r merged_model.zip {save_directory}\n",
        "files.download('merged_model.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFhROXXsqJjx"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "save_directory = \"/content/drive/MyDrive/oscilloscope_project/qwen_merged_hf\"\n",
        "print(f\"正在从 {save_directory} 加载合并后的模型...\")\n",
        "\n",
        "# 由于是合并后的模型，可以直接加载为标准的 Hugging Face 模型\n",
        "# 注意：此处使用的 FastLanguageModel.from_pretrained 是针对 Unsloth 优化过的模型加载方式\n",
        "# 对于完全合并的模型，也可以使用 AutoModelForCausalLM.from_pretrained\n",
        "# 但为了保持一致性，我们继续使用 FastLanguageModel\n",
        "merged_model, merged_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = save_directory, # 指向保存合并模型的目录\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = None, # 自动检测数据类型\n",
        "    load_in_4bit = False, # 合并后的模型通常不需要再进行 4bit 量化加载\n",
        ")\n",
        "\n",
        "print(\"✅ 合并模型加载完成！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcb298b0"
      },
      "source": [
        "#用CPU测试\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "save_directory = \"/content/drive/MyDrive/oscilloscope_project/qwen_merged_hf\"\n",
        "print(f\"正在从 {save_directory} 加载合并后的模型到 CPU...\")\n",
        "\n",
        "# 对于完全合并的模型，我们可以使用 AutoModelForCausalLM 和 AutoTokenizer\n",
        "# 显式指定 device_map='cpu' 来在 CPU 上加载\n",
        "merged_model = AutoModelForCausalLM.from_pretrained(\n",
        "    save_directory,\n",
        "    torch_dtype=torch.float32, # CPU 上通常使用 float32\n",
        "    device_map=\"cpu\",\n",
        ")\n",
        "merged_tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "\n",
        "print(\"✅ 合并模型已成功加载到 CPU！\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae1b61ab"
      },
      "source": [
        "# CPU测试合并后的模型\n",
        "# 在 CPU 上运行时，无需 FastLanguageModel.for_inference\n",
        "\n",
        "# 必须使用与训练时完全一致的 Prompt 模板\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = merged_tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"你是一个工业级示波器指令解析引擎。你必须只输出 JSON，不得包含任何解释性文本。\",\n",
        "        \"将触发源改为外部\",\n",
        "        \"\", # 这里留空，让模型生成响应\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cpu\") # 将输入也移动到 CPU\n",
        "\n",
        "outputs = merged_model.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "# 解码时跳过 prompt 部分\n",
        "print(merged_tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d956e194"
      },
      "source": [
        "#GPU 测试合并后的模型\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# 定义保存合并模型的目录\n",
        "save_directory = \"/content/drive/MyDrive/oscilloscope_project/qwen_merged_hf\"\n",
        "print(f\"正在从 {save_directory} 加载合并后的模型到 GPU...\")\n",
        "\n",
        "# 使用 FastLanguageModel 加载合并后的模型\n",
        "# 确保运行时类型已设置为 GPU\n",
        "merged_model_gpu, merged_tokenizer_gpu = FastLanguageModel.from_pretrained(\n",
        "    model_name = save_directory, # 指向保存合并模型的目录\n",
        "    max_seq_length = 2048, # 与训练时保持一致\n",
        "    dtype = None, # FastLanguageModel 会自动检测并使用最佳数据类型\n",
        "    load_in_4bit = False, # 合并后的模型通常不需要再进行 4bit 量化加载\n",
        ")\n",
        "\n",
        "print(\"✅ 合并模型已成功加载到 GPU！\")\n",
        "\n",
        "FastLanguageModel.for_inference(merged_model_gpu)\n",
        "\n",
        "# 必须使用与训练时完全一致的 Prompt 模板\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "inputs = merged_tokenizer_gpu(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"你是一个工业级示波器指令解析引擎。你必须只输出 JSON，不得包含任何解释性文本。\",\n",
        "        \"将触发源改为外部\",\n",
        "        \"\", # 这里留空，让模型生成响应\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\") # 将输入移动到 GPU\n",
        "\n",
        "outputs = merged_model_gpu.generate(**inputs, max_new_tokens = 128, use_cache = True)\n",
        "# 解码时跳过 prompt 部分\n",
        "print(merged_tokenizer_gpu.decode(outputs[0], skip_special_tokens=True))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}